{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758a37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hw = 416\n",
    "frame_w = 1280\n",
    "frame_h = 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0eb0dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/xilinx/')\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tflite_runtime.interpreter as tflite\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pynq\n",
    "\n",
    "from pynq import Overlay\n",
    "from pynq.lib.video import *\n",
    "\n",
    "from tcu_pynq.driver import Driver\n",
    "from tcu_pynq.util import div_ceil\n",
    "from tcu_pynq.architecture import ultra96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e58e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = Overlay('/home/xilinx/tensil_ultra96v2.bit')\n",
    "tcu = Driver(ultra96, overlay.axi_dma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9428cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_w);\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_h);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a0880ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "displayport = DisplayPort()\n",
    "displayport.configure(VideoMode(frame_w, frame_h, 24), PIXEL_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a398d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcu.load_model('/home/xilinx/yolov4_tiny_{0}_onnx_ultra96v2.tmodel'.format(model_hw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ec2c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tflite.Interpreter(model_path='/home/xilinx/yolov4_tiny_{0}_post.tflite'.format(model_hw))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4dcb9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/xilinx/coco-labels-2014_2017.txt') as f:\n",
    "    labels_coco = f.read().split('\\n')\n",
    "    \n",
    "def set_tensor(driver, interpreter, hw_size, data):\n",
    "    input_details = interpreter.get_input_details()\n",
    "    input_idxs = [i for i in range(len(input_details))\n",
    "                  if input_details[i]['shape'][1] == hw_size and input_details[i]['shape'][2] == hw_size]\n",
    "    inp = input_details[input_idxs[0]]\n",
    "    data = data.astype(inp['dtype'])\n",
    "    inner_dim = inp['shape'][-1]\n",
    "    inner_size = div_ceil(inner_dim, driver.arch.array_size) * driver.arch.array_size\n",
    "    if inner_size != inner_dim:\n",
    "        data = data.reshape((-1, inner_size))[:, :inner_dim]\n",
    "    data = data.reshape(inp['shape'])\n",
    "    interpreter.set_tensor(inp['index'], data)\n",
    "    \n",
    "def filter_and_reshape(boxes, scores, score_threshold=0.4):\n",
    "    scores_max = np.max(scores, axis=-1)\n",
    "    mask = scores_max > score_threshold\n",
    "    \n",
    "    filtered_boxes = boxes[mask]\n",
    "    filtered_scores = scores[mask]\n",
    "    \n",
    "    filtered_boxes = np.reshape(filtered_boxes, [scores.shape[0], -1, filtered_boxes.shape[-1]])    \n",
    "    filtered_scores = np.reshape(filtered_scores, [scores.shape[0], -1, filtered_scores.shape[-1]])\n",
    "\n",
    "    return filtered_boxes, filtered_scores\n",
    "\n",
    "\n",
    "def non_maximum_suppression(boxes, iou_threshold=0.4):\n",
    "    if len(boxes) == 0:\n",
    "        return boxes\n",
    "    \n",
    "    area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "    ll_x = np.maximum.outer(boxes[:, 0], boxes[:, 0])\n",
    "    ll_y = np.maximum.outer(boxes[:, 1], boxes[:, 1])\n",
    "    ur_x = np.minimum.outer(boxes[:, 2], boxes[:, 2])\n",
    "    ur_y = np.minimum.outer(boxes[:, 3], boxes[:, 3])\n",
    "    intersection_x = np.maximum(0, ur_x - ll_x)\n",
    "    intersection_y = np.maximum(0, ur_y - ll_y)\n",
    "    intersection = intersection_x * intersection_y\n",
    "    \n",
    "    iou = intersection / area - np.identity(area.shape[-1])\n",
    "    p = iou >= iou_threshold\n",
    "    p = p & p.T\n",
    "    n =  p.shape[-1]\n",
    "    \n",
    "    no_needs_merge = set()\n",
    "    for i in range(n):\n",
    "        if not p[i].any():\n",
    "            no_needs_merge.add(i)\n",
    "    \n",
    "    needs_merge = set()\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if p[i, j]:\n",
    "                needs_merge.add(tuple(sorted((i, j))))\n",
    "\n",
    "    def merge(needs_merge):\n",
    "        result = set()\n",
    "        discarded = set()\n",
    "        for indices in needs_merge:\n",
    "            idx = indices[0]\n",
    "            if idx not in discarded:\n",
    "                result.add(indices[0])\n",
    "            discarded.add(indices[1])\n",
    "            if indices[1] in result:\n",
    "                result.remove(indices[1])\n",
    "        return result\n",
    "\n",
    "    return sorted(list(no_needs_merge) + list(merge(needs_merge)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3786d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cv2.VideoWriter('/home/xilinx/recording.mp4',cv2.VideoWriter_fourcc(*\"MJPG\"),1,(frame_w,frame_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08239570",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _ in range(60):\n",
    "    start = time.time()\n",
    "    \n",
    "    cap_frame = displayport.newframe()\n",
    "    cap.read(cap_frame)\n",
    "    \n",
    "    crop_h = int(max(0, (frame_h - frame_w) / 2))\n",
    "    crop_w = int(max(0, (frame_w - frame_h) / 2))\n",
    "    ratio_h = (frame_h - crop_h * 2)/model_hw\n",
    "    ratio_w = (frame_w - crop_w * 2)/model_hw\n",
    "\n",
    "    x_frame = cap_frame    \n",
    "    x_frame=x_frame[crop_h:frame_h - crop_h, crop_w:frame_w - crop_w]\n",
    "    x_frame=cv2.resize(x_frame, (model_hw, model_hw), interpolation=cv2.INTER_LINEAR)\n",
    "    x_frame=cv2.cvtColor(x_frame, cv2.COLOR_BGR2RGB)    \n",
    "    x_frame = x_frame.astype('float32') / 255\n",
    "    x_frame = np.pad(x_frame, [(0, 0), (0, 0), (0, tcu.arch.array_size - 3)], 'constant', constant_values=0)\n",
    "    \n",
    "    inputs = {'x:0': x_frame}    \n",
    "    outputs = tcu.run(inputs)\n",
    "    \n",
    "    set_tensor(tcu, interpreter, model_hw / 32, np.array(outputs['model/conv2d_17/BiasAdd:0']))\n",
    "    set_tensor(tcu, interpreter, model_hw / 16, np.array(outputs['model/conv2d_20/BiasAdd:0']))\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_details = interpreter.get_output_details()\n",
    "    scores, boxes_xywh = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n",
    "\n",
    "    boxes_xywh, scores = filter_and_reshape(boxes_xywh, scores)\n",
    "    \n",
    "    boxes_xy, boxes_wh = np.split(boxes_xywh, (2,), axis=-1)\n",
    "    boxes_x0y0x1y1 = np.concatenate([boxes_xy - boxes_wh/2, boxes_xy + boxes_wh/2], axis=-1)\n",
    "    \n",
    "    box_indices = non_maximum_suppression(boxes_x0y0x1y1[0])\n",
    "\n",
    "    latency = (time.time() - start)\n",
    "    fps = 1/latency\n",
    "    \n",
    "    for i in box_indices:\n",
    "        category_idx = np.argmax(scores, axis=-1)[0, i]\n",
    "        category_conf = np.max(scores, axis=-1)[0, i]\n",
    "        text = f'{labels_coco[category_idx]} = {category_conf:.2}'\n",
    "        \n",
    "        box_x0y0x1y1 = boxes_x0y0x1y1[0, i]        \n",
    "        box_x0y0x1y1[0] *= ratio_w\n",
    "        box_x0y0x1y1[1] *= ratio_h\n",
    "        box_x0y0x1y1[2] *= ratio_w\n",
    "        box_x0y0x1y1[3] *= ratio_h\n",
    "        box_x0y0x1y1[0] += crop_w\n",
    "        box_x0y0x1y1[1] += crop_h\n",
    "        box_x0y0x1y1[2] += crop_w\n",
    "        box_x0y0x1y1[3] += crop_h\n",
    "        box_x0y0x1y1 = box_x0y0x1y1.astype('int')\n",
    "        \n",
    "        cap_frame = cv2.rectangle(cap_frame, (crop_w, crop_h), (frame_w - crop_w, frame_h - crop_h), (255, 0, 0), 1)\n",
    "        cap_frame = cv2.rectangle(cap_frame, (box_x0y0x1y1[0], box_x0y0x1y1[1]), (box_x0y0x1y1[2], box_x0y0x1y1[3]), (0, 0, 255), 1)\n",
    "        cap_frame = cv2.putText(cap_frame, text, (box_x0y0x1y1[0] + 2, box_x0y0x1y1[1] - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255))\n",
    "            \n",
    "    \n",
    "    cap_frame = cv2.putText(cap_frame, f'{fps:.2}fps', (2, frame_h - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0))\n",
    "    output.write(cap_frame.copy())\n",
    "    displayport.writeframe(cap_frame)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63c6bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.release()\n",
    "displayport.close()\n",
    "cap.release()\n",
    "tcu.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76db661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57943d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
